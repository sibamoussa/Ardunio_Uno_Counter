{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SKLearn Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smoussa24/Ardunio_Uno_Counter/blob/master/SKLearn_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XJCCUgRky5b-"
      },
      "source": [
        "# McGill AI Scikit-Learn Workshop\n",
        "\n",
        "January 27, 2020\n",
        "\n",
        "**Etienne Denis**\n",
        "\n",
        "Slideshow url: [git.io/fh91K](git.io/fh91K)\n",
        "\n",
        "Colab: [here](https://colab.research.google.com/drive/1LQuuM9oNuQhX16jyMoD2ekkIvJ4nefHd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DWojGlHdy5b-"
      },
      "source": [
        "## Why SKLearn\n",
        "- Implementing scalable and efficient ML models is **hard**\n",
        "- Python packages ecosystem\n",
        "- Opensource and well documented\n",
        "- Includes many models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kjxtPBSvy5cB"
      },
      "source": [
        "## (Supervised) ML Pipeline\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1rRN74OATkfsAJPrUmT1YxxXeA9lQK5BN\" width=\"1000\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f3AiQWOLy5cC"
      },
      "source": [
        "## ML with SKLearn \n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1BhV_PosHmARa0SKJ18OzbY9flD0hMXJI\" width=\"1000\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsxER2wlRbTV",
        "colab_type": "text"
      },
      "source": [
        "## NLP Example: 20 News Group Dataset\n",
        "We will explain the concepts behind each of these steps using the [ 20 News Group dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "klEFnYucy5cD",
        "outputId": "cd9e4626-e46a-46b3-e2d3-f401c8ee82e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups = fetch_20newsgroups(subset='all')\n",
        "\n",
        "# The 20 News Group dataset is a collection of ~20,000 newsgroup documents \n",
        "# partitioned into 20 balanced categories (\"groups\") as shown below:\n",
        "\n",
        "newsgroups.target_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2o_TjhERbTc",
        "colab_type": "text"
      },
      "source": [
        "## Splitting the data\n",
        "* SKLearn takes care of partitioning data with the function [`train_test_split` ](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vX07SHuvy5cP",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, train_size=0.8, test_size=0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYevr0fPfZZv",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhO0SkcufZKW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWqfNcMSRbTj",
        "colab_type": "text"
      },
      "source": [
        "## Vectorization and Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ppUUSZ-My5cS",
        "outputId": "c1e585f2-7d9d-449d-c2bd-4a9956933a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        }
      },
      "source": [
        "# Always look at the data before making feature engineering decisions\n",
        "# Seems like the internet never changes...\n",
        "print(X_train[10000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: mark@taylor.uucp (Mark A. Davis)\n",
            "Subject: Re: Blinking Cursor in Xterm???\n",
            "Organization: Lake Taylor Hospital Computer Services\n",
            "Lines: 25\n",
            "\n",
            "barmar@think.com (Barry Margolin) writes:\n",
            "\n",
            ">In article <1993May18.130845.6859@taylor.uucp> mark@taylor.uucp (Mark A. Davis) writes:\n",
            ">>You are stuck in a distributed system feedback loop!  What if you are on an\n",
            ">>Xterminal or running xterm over the net to another machine?  That is when the\n",
            ">>load problems occur.  If you had a machine with 20 Xterminals attached and\n",
            ">>they all had blinking cursors in xterm's, this could represent a considerable\n",
            ">>ethernet bandwidth waste.\n",
            "\n",
            ">Ethernet certainly has enough bandwidth for a few blinking cursors.  My\n",
            ">Lisp Machine blinks the cursor twice a second and updates the time in the\n",
            ">status line every second.  This uses under 1K bytes/second, or about .1% of\n",
            ">the bandwidth of Ethernet.  A hundred of them on a single ethernet might be\n",
            ">a problem, but a couple dozen should be fine.\n",
            "\n",
            "Granted it's nothing to loose sleep over, but this is Ethernet's tragic flaw:\n",
            "the more activity (especially lots of tiny activity), the more collisions\n",
            "happen and the performance gets exponentially worse...  I am just now\n",
            "opposing ANY kind of waste of bandwidth under Ethernet.  Although in a polling\n",
            "system it would not be so bad.\n",
            "-- \n",
            "  /--------------------------------------------------------------------------\\\n",
            "  | Mark A. Davis    | Lake Taylor Hospital | Norfolk, VA (804)-461-5001x431 |\n",
            "  | Sys.Administrator|  Computer Services   | mark@taylor / mark@taylor.UUCP |\n",
            "  \\--------------------------------------------------------------------------/\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "24q_UHoUy5ca"
      },
      "source": [
        "## Bag of Words\n",
        "\n",
        "Each word in the selected vocabulary (e.g. `{apple, bannana}`) is **one hot encoded**:\n",
        "```\n",
        "Apple  ==>> [1,0]\n",
        "Banana ==>> [0,1]\n",
        "```\n",
        "\n",
        "A BoW vectorized “document” encodes the multiplicity of each (vocab) word in the document:\n",
        "```\n",
        "d1 = \"Apple Banana Banana Apple\" ==>> [2,2]\n",
        "d2 = \"Banana Cat Chicken Frog\" ==>> [1,0]\n",
        "d3 = \"Banana Apple Banana Apple\" ==>> [2,2]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTiRLMeRRbTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer().fit(X_train)\n",
        "X_train_counts = count_vect.transform(X_train)\n",
        "X_test_counts = count_vect.transform(X_test) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK4vVFzURbTy",
        "colab_type": "text"
      },
      "source": [
        "There are many other ways of vectorizing textual features. Most methods result in (lossy) compression of information present in the text. What's lost has a large impact on downstream performance. \n",
        "\n",
        "**Think about what sort of information is lost with a BoW encoding?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eKljh0LHy5cb"
      },
      "source": [
        "## Better Features\n",
        "* Bag of words loses information on ordering and relative frequency\n",
        "    * tf-idf\n",
        "    * n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0opTm5sxy5cd",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer().fit(X_train_counts)\n",
        "X_train_tfidf = tfidf_transformer.transform(X_train_counts)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FMp472oFy5c1"
      },
      "source": [
        "## Normalization\n",
        "* Scales individual samples (rows of `X_train`) to have unit norm \n",
        "* Many way of normalizing, SKLearn provides (l1,l2,max)\n",
        "* We'll use the default **l2 normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3-7AWYey5c3",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "normalizer_tranformer = Normalizer().fit(X=X_train_tfidf)\n",
        "X_train_normalized = normalizer_tranformer.transform(X_train_tfidf)\n",
        "X_test_normalized = normalizer_tranformer.transform(X_test_tfidf)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N2rYWqWRbUD",
        "colab_type": "text"
      },
      "source": [
        "## The Model\n",
        "* SKLearn provides a wide variety of [models](https://scikit-learn.org/stable/supervised_learning.html)\n",
        "* No gpu support\n",
        "* Not a deeplearning library\n",
        "<img src=http://scikit-learn.org/stable/_static/ml_map.png width=\"1000\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jxif9C3my5c7",
        "colab": {}
      },
      "source": [
        "# We will by using MultinomialNB but there are many other good choices\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_normalized, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "osRWgUIMy5dI"
      },
      "source": [
        "##  Prediction and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ4ftpnjRbUP",
        "colab_type": "code",
        "outputId": "b8580766-f5e7-4d91-c97b-c76c9998b37e",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "y_pred = clf.predict(X_test_normalized)\n",
        "print(metrics.classification_report(y_test, y_pred,\n",
        "    target_names=newsgroups.target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.90      0.73      0.80       178\n",
            "           comp.graphics       0.85      0.74      0.79       192\n",
            " comp.os.ms-windows.misc       0.86      0.84      0.85       208\n",
            "comp.sys.ibm.pc.hardware       0.71      0.91      0.79       182\n",
            "   comp.sys.mac.hardware       0.93      0.82      0.87       188\n",
            "          comp.windows.x       0.95      0.82      0.88       212\n",
            "            misc.forsale       0.90      0.70      0.79       192\n",
            "               rec.autos       0.86      0.89      0.87       186\n",
            "         rec.motorcycles       0.98      0.96      0.97       209\n",
            "      rec.sport.baseball       0.97      0.95      0.96       201\n",
            "        rec.sport.hockey       0.91      0.99      0.95       206\n",
            "               sci.crypt       0.74      0.98      0.84       192\n",
            "         sci.electronics       0.89      0.72      0.80       195\n",
            "                 sci.med       0.97      0.93      0.95       197\n",
            "               sci.space       0.89      0.97      0.93       186\n",
            "  soc.religion.christian       0.61      0.97      0.75       216\n",
            "      talk.politics.guns       0.66      0.98      0.79       162\n",
            "   talk.politics.mideast       0.86      0.96      0.91       182\n",
            "      talk.politics.misc       0.99      0.57      0.73       160\n",
            "      talk.religion.misc       1.00      0.19      0.32       126\n",
            "\n",
            "               micro avg       0.85      0.85      0.85      3770\n",
            "               macro avg       0.87      0.83      0.83      3770\n",
            "            weighted avg       0.87      0.85      0.84      3770\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdCX7qKKRbUY",
        "colab_type": "text"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaUYdw-DRbUZ",
        "colab_type": "text"
      },
      "source": [
        "#### Notice how there is a repetitive structure our code:\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1D7Fc3Cv5rBl9-XvaVpCdtSGLs4u-knJu\" width=\"1000\">\n",
        "\n",
        "#### This calls for abstraction!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jvqyFeOpy5dL",
        "colab": {}
      },
      "source": [
        "## SKLearn Pipeline\n",
        "Why?\n",
        "* abstraction\n",
        "* k-fold cross validation\n",
        "* keeping track of hyperparameters\n",
        "\n",
        "How?\n",
        " * Chains a series of transformers followed by an estimator passing data from one program to another"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp9gOI96RbUf",
        "colab_type": "text"
      },
      "source": [
        "## SKLearn Pipeline\n",
        "![](http://docs.google.com/drawings/d/e/2PACX-1vQjzopDKBYU5J0vlQma0a3nQ_6KQulscS5P3CYlWEhAIVey3Ofj1U7_f4QRZvE4W0Nzaq7AYdzQLpOD/pub?w=463&h=531)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vxsozhnqy5dR"
      },
      "source": [
        "## Pipeline Makeover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j9N0dW68y5dR",
        "outputId": "c4a2bc76-7456-4ff0-be45-ab29a37289e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "pclf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('norm', Normalizer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "\n",
        "pclf.fit(X_train, y_train)\n",
        "y_pred = pclf.predict(X_test)\n",
        "print(metrics.classification_report(y_test, y_pred,\n",
        "    target_names=newsgroups.target_names))\n",
        "\n",
        "# That was easy...."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.90      0.73      0.80       178\n",
            "           comp.graphics       0.85      0.74      0.79       192\n",
            " comp.os.ms-windows.misc       0.86      0.84      0.85       208\n",
            "comp.sys.ibm.pc.hardware       0.71      0.91      0.79       182\n",
            "   comp.sys.mac.hardware       0.93      0.82      0.87       188\n",
            "          comp.windows.x       0.95      0.82      0.88       212\n",
            "            misc.forsale       0.90      0.70      0.79       192\n",
            "               rec.autos       0.86      0.89      0.87       186\n",
            "         rec.motorcycles       0.98      0.96      0.97       209\n",
            "      rec.sport.baseball       0.97      0.95      0.96       201\n",
            "        rec.sport.hockey       0.91      0.99      0.95       206\n",
            "               sci.crypt       0.74      0.98      0.84       192\n",
            "         sci.electronics       0.89      0.72      0.80       195\n",
            "                 sci.med       0.97      0.93      0.95       197\n",
            "               sci.space       0.89      0.97      0.93       186\n",
            "  soc.religion.christian       0.61      0.97      0.75       216\n",
            "      talk.politics.guns       0.66      0.98      0.79       162\n",
            "   talk.politics.mideast       0.86      0.96      0.91       182\n",
            "      talk.politics.misc       0.99      0.57      0.73       160\n",
            "      talk.religion.misc       1.00      0.19      0.32       126\n",
            "\n",
            "               micro avg       0.85      0.85      0.85      3770\n",
            "               macro avg       0.87      0.83      0.83      3770\n",
            "            weighted avg       0.87      0.85      0.84      3770\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL_2pO-tRbUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From: https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html\n",
        "import numpy as np\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                  results['mean_test_score'][candidate],\n",
        "                  results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqeCemiuRbUt",
        "colab_type": "text"
      },
      "source": [
        "## Randomized Search and Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A0lwFEchy5dw",
        "outputId": "e77d9c4b-d857-45f8-f877-ae01e370eee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint as randint\n",
        "from scipy.stats import uniform\n",
        "\n",
        "params = {\"vect__ngram_range\": [(1,1),(1,2),(2,2)],\n",
        "          \"tfidf__use_idf\": [True, False],\n",
        "          \"clf__alpha\": uniform(1e-2, 1e-3)}\n",
        "\n",
        "seed = 551 # Very important for repeatibility in experiments!\n",
        "\n",
        "random_search = RandomizedSearchCV(pclf, param_distributions = params, cv=2, verbose = 10, random_state = seed, n_iter = 1)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
            "[CV] clf__alpha=0.010640064047894703, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  clf__alpha=0.010640064047894703, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.8775347912524851, total=   7.6s\n",
            "[CV] clf__alpha=0.010640064047894703, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.3s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  clf__alpha=0.010640064047894703, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.8834152171026424, total=   6.9s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   21.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   21.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=2, error_score='raise-deprecating',\n",
              "          estimator=Pipeline(memory=None,\n",
              "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "        strip...malizer(copy=True, norm='l2')), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
              "          fit_params=None, iid='warn', n_iter=1, n_jobs=None,\n",
              "          param_distributions={'vect__ngram_range': [(1, 1), (1, 2), (2, 2)], 'tfidf__use_idf': [True, False], 'clf__alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x11d990b38>},\n",
              "          pre_dispatch='2*n_jobs', random_state=551, refit=True,\n",
              "          return_train_score='warn', scoring=None, verbose=10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1IU20LERbU1",
        "colab_type": "text"
      },
      "source": [
        "## CV Results and Final Eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkrtHLybRbU1",
        "colab_type": "code",
        "outputId": "f5758655-f87a-4e73-8d07-0eb16ba23c10",
        "colab": {}
      },
      "source": [
        "report(random_search.cv_results_)\n",
        "y_pred = random_search.predict(X_test)\n",
        "print(metrics.classification_report(y_test, y_pred,\n",
        "    target_names=newsgroups.target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model with rank: 1\n",
            "Mean validation score: 0.880 (std: 0.003)\n",
            "Parameters: {'clf__alpha': 0.010640064047894703, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
            "\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.92      0.90      0.91       178\n",
            "           comp.graphics       0.77      0.84      0.80       192\n",
            " comp.os.ms-windows.misc       0.87      0.86      0.86       208\n",
            "comp.sys.ibm.pc.hardware       0.78      0.87      0.82       182\n",
            "   comp.sys.mac.hardware       0.89      0.88      0.89       188\n",
            "          comp.windows.x       0.94      0.89      0.91       212\n",
            "            misc.forsale       0.88      0.88      0.88       192\n",
            "               rec.autos       0.93      0.92      0.93       186\n",
            "         rec.motorcycles       0.99      0.96      0.97       209\n",
            "      rec.sport.baseball       0.99      0.96      0.97       201\n",
            "        rec.sport.hockey       0.95      0.99      0.97       206\n",
            "               sci.crypt       0.96      0.97      0.96       192\n",
            "         sci.electronics       0.90      0.87      0.89       195\n",
            "                 sci.med       0.96      0.95      0.96       197\n",
            "               sci.space       0.96      0.97      0.97       186\n",
            "  soc.religion.christian       0.85      0.97      0.91       216\n",
            "      talk.politics.guns       0.84      0.98      0.90       162\n",
            "   talk.politics.mideast       0.94      0.96      0.95       182\n",
            "      talk.politics.misc       0.95      0.84      0.89       160\n",
            "      talk.religion.misc       0.96      0.58      0.72       126\n",
            "\n",
            "               micro avg       0.91      0.91      0.91      3770\n",
            "               macro avg       0.91      0.90      0.90      3770\n",
            "            weighted avg       0.91      0.91      0.91      3770\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izpuYfUYRbU4",
        "colab_type": "text"
      },
      "source": [
        "# Customizing the Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4qQKCyiRbU5",
        "colab_type": "text"
      },
      "source": [
        "## Custom Transformers\n",
        "Assignment 1 asked you to create custom features. How could we add similar features to our SKLearn pipeline? We'll be working off of [an example in the SKLearn docs](https://scikit-learn.org/0.18/auto_examples/hetero_feature_union.html).\n",
        "\n",
        "We'll have to create our own classes of the same polymorphic type that the [**Pipeline** class](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) expects. Estimators (i.e. ML Models) and Transformers both extend two base classes in SKLearn:\n",
        "\n",
        "* *Transformers and estimators* both extend the [**BaseEstimator** class](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)\n",
        "\n",
        "* *Transformers* also extend the [**TransformerMixin** class](https://sklearn.org/modules/generated/sklearn.base.TransformerMixin.html)\n",
        "\n",
        "* *Estimators* also extend the [**ClassifierMixin** class](https://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html)\n",
        "\n",
        "Let's take another look at a NewsGroup document to inform our new feature choices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dr2nqXoRbU6",
        "colab_type": "code",
        "outputId": "bf5edcea-5e40-4a65-aa79-bbd6f61b7a3e",
        "colab": {}
      },
      "source": [
        "# Notice the seperate subject and body. The subject carries concise information \n",
        "# that is lost by performing tfidf BoW on the whole. \n",
        "print(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: jason@ab20.larc.nasa.gov (Jason Austin)\n",
            "Subject: Polls (was Re: Top Ten Excuses for Slick Willie's Record-Setting Disapproval Rati)\n",
            "Organization: NASA Langley Research Center, Hampton, VA\n",
            "Lines: 37\n",
            "Reply-To: Jason C. Austin <j.c.austin@larc.nasa.gov>\n",
            "NNTP-Posting-Host: ab20.larc.nasa.gov\n",
            "In-reply-to: libwca@emory.edu's message of 14 Apr 93 20:31:09 GMT\n",
            "\n",
            "In article <2680@emoryu1.cc.emory.edu> libwca@emory.edu (Bill Anderson) writes:\n",
            "-> : \tAccording to a ``CNN Poll'' to key reason for Clinton's low\n",
            "-> : approval rating is people are angry about him not moving fast enough\n",
            "-> : on gays in the military.  I just burst out laughing when I heard this;\n",
            "-> : what planet do these CNN people live on anyway?\n",
            "-> : --\n",
            "-> : Jason C. Austin\n",
            "-> : j.c.austin@larc.nasa.gov       \n",
            "-> \n",
            "-> Dunno, man... that sounds pretty damned unlikely to me, too,\n",
            "-> although it's certainly one of the reasons I'm pissed off at him.\n",
            "-> Maybe the sample was taken entirely from my fellow memebers of the\n",
            "-> Cultural Elite?\n",
            "-> \n",
            "-> Jason, can you quote some of these poll questions?\n",
            "-> \n",
            "-> Thanks,\n",
            "-> Bill\n",
            "-> v\n",
            "\n",
            "\tI've never seen CNN give out the poll questions on the air.\n",
            "If you sent them a letter asking for them, you might get them.  Here's\n",
            "my guess of how part of a session might look:\n",
            "\n",
            "Question: Do you approve of Clinton's performance?\n",
            "Answer: No\n",
            "Questions: Do you disapprove due to the gays in the military issue?\n",
            "Answer: Yes\n",
            "\n",
            "Conclusion: Clinton has a low approval rating because he's not moving\n",
            "fast enough on gays in the military.\n",
            "\n",
            "\n",
            "\tI think any group truly dedicated to reporting the news would\n",
            "not use manufactured news like polls.\n",
            "\n",
            "\t\t\t\t\t\t-Jason\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIW8RvUQRbU9",
        "colab_type": "text"
      },
      "source": [
        "As seen in the document we might gain from looking at the subject and body seperately.\n",
        "\n",
        "We'll be adding **three custom features**:\n",
        "* BoW for the *subject line*\n",
        "* BoW for the *body* \n",
        "* Vector of body *text stats*\n",
        "    * length of text\n",
        "    * number of sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT3QtXE6RbU-",
        "colab_type": "text"
      },
      "source": [
        "The pipeline passes data in a sequential manner from transformer to transformer (and finally to an estimator). Let's get a *rough* idea of what the flow looks like in our case:\n",
        "\n",
        "* Splitting the subject and body\n",
        "* Calculating the custom *text stats*\n",
        "* Calculating BoW features for *subject line* and *body*\n",
        "\n",
        "Finally we will also have to consider how to merge all of these features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxbLyoZmRbU_",
        "colab_type": "text"
      },
      "source": [
        "## Splitting the Subject and Body\n",
        "* We will seperate the documents into a dictionary that contains `subject` and `body` keys, so that dowstream transformers can access the seperated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp6Vw6fjRbVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
        "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.base import TransformerMixin\n",
        "\n",
        "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
        "\n",
        "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
        "    `subject` and `body`.\n",
        "    \"\"\"\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, posts):\n",
        "        features = np.recarray(shape=(len(posts),),\n",
        "                               dtype=[('subject', object), ('body', object)])\n",
        "        for i, text in enumerate(posts):\n",
        "            headers, _, bod = text.partition('\\n\\n')\n",
        "            bod = strip_newsgroup_footer(bod)\n",
        "            bod = strip_newsgroup_quoting(bod)\n",
        "            features['body'][i] = bod\n",
        "\n",
        "            prefix = 'Subject:'\n",
        "            sub = ''\n",
        "            for line in headers.split('\\n'):\n",
        "                if line.startswith(prefix):\n",
        "                    sub = line[len(prefix):]\n",
        "                    break\n",
        "            features['subject'][i] = sub\n",
        "\n",
        "        return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir6_Hs1wRbVF",
        "colab_type": "text"
      },
      "source": [
        "## Selecting the Right Data\n",
        "* Remember that all data manipulation within the pipeline must be by a transformer that extends the proper class, even for something as simple as selecting a dictionary key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEYpn75LRbVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
        "\n",
        "    The data is expected to be stored in a 2D data structure, where the first\n",
        "    index is over features and the second is over samples.  i.e.\n",
        "\n",
        "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
        "               'b': [9, 4, 1, 4, 1, 3]}\n",
        "    >> ds = ItemSelector(key='a')\n",
        "    >> data['a'] == ds.transform(data)\n",
        "    \"\"\"\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BqStF62RbVJ",
        "colab_type": "text"
      },
      "source": [
        "## Nested Pipelines\n",
        "* To simplify working with more complicated flows, **pipelines can be nested** (we'll talk about how later)\n",
        "* We have the necessary components for text and body BoW features. *Let's extract these features*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfLHBQ_7RbVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "# Pipeline for pulling features from the post's subject line\n",
        "subject_pipeline =  Pipeline([\n",
        "    ('selector', ItemSelector(key='subject')),\n",
        "    ('tfidf', TfidfVectorizer(min_df=50)),\n",
        "])\n",
        "\n",
        "# Pipeline for standard bag-of-words model for body\n",
        "body_pipeline = Pipeline([\n",
        "    ('selector', ItemSelector(key='body')),\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('best', TruncatedSVD(n_components=50)),\n",
        "    # TruncatedSVD simply does dimensionality reduction\n",
        "    # We are picking the \"n_components\" most informative words\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz4v_gwKRbVM",
        "colab_type": "text"
      },
      "source": [
        "## Text Stats Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA0KO7KIRbVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextStats(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, posts):\n",
        "        return [{'length': len(text),\n",
        "                 'num_sentences': text.count('.')}\n",
        "                for text in posts]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyQfmnD6RbVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "#Pipeline for pulling ad hoc features from post's body\n",
        "body_stats_pipeline =  Pipeline([\n",
        "    ('selector', ItemSelector(key='body')),\n",
        "    ('stats', TextStats()),  # returns a list of dicts\n",
        "    ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zXZJsbgRbVV",
        "colab_type": "text"
      },
      "source": [
        "## Combining Features\n",
        "* **Remember:** Data flows sequentially in the pipeline. \n",
        "\n",
        "How do we go about extracting several features in parallel from the same data?\n",
        "   * SKLearn provides a [**FeatureUnion** class](https://scikit-learn.org/0.18/modules/generated/sklearn.pipeline.FeatureUnion.html) that allows us to pass the data from the previous step to several nested pipelines.\n",
        "   * FeatureUnion works by **concatenating** our feature vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olvIrer-RbVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseTransformer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None, **fit_params):\n",
        "        return X.toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKtBijzaRbVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "custom_features = FeatureUnion(\n",
        "        transformer_list=[\n",
        "            # The custom feature transformers\n",
        "            ('subject', subject_pipeline),\n",
        "            ('body_bow', body_pipeline),\n",
        "            ('body_stats', body_stats_pipeline),\n",
        "        ],\n",
        "    \n",
        "        # weight feature components in FeatureUnion\n",
        "        transformer_weights={\n",
        "            'subject': 0.8,\n",
        "            'body_bow': 0.5,\n",
        "            'body_stats': 1.0,\n",
        "        },\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byVYTTXkRbVd",
        "colab_type": "text"
      },
      "source": [
        "## Putting it all together\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCLKHsErRbVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "custom_feature_pipeline = Pipeline([\n",
        "    # Extract the subject & body\n",
        "    ('subjectbody', SubjectBodyExtractor()),\n",
        "\n",
        "    # Use FeatureUnion to combine the features from subject and body\n",
        "    ('union', custom_features),\n",
        "    # Use a SVC classifier on the combined features\n",
        "    ('svc', SVC(kernel='linear'))\n",
        "\n",
        "])\n",
        "\n",
        "subj = SubjectBodyExtractor().fit(X_train)\n",
        "custom_feature_pipeline.fit(X_train, y = y_train)\n",
        "y_pred = custom_feature_pipeline.predict(y_test)\n",
        "print(metrics.classification_report(y_pred, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APqJ5uraRbVf",
        "colab_type": "text"
      },
      "source": [
        "## Good Practices\n",
        "* Python can encourage lousy programming\n",
        "* Make sure your experiments are repeatible \n",
        "    * random seeds\n",
        "    * abstraction\n",
        "    * recording parameters and outputs\n",
        "* When using jupyter be careful for namespace bugs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BASQgLITRbVk",
        "colab_type": "text"
      },
      "source": [
        "## Questions?\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1BhV_PosHmARa0SKJ18OzbY9flD0hMXJI\" width=\"400\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXI7LQ9cUZHN",
        "colab_type": "text"
      },
      "source": [
        "**Stay up to date with all of our events! Signup for the weekly newsletter** [here](https://docs.google.com/forms/d/e/1FAIpQLScs1ADlyTZHnvbQPIftXhE2yGvqPGzyhhKq87q212ttkOmIaQ/viewform)"
      ]
    }
  ]
}